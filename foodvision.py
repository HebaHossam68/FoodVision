# -*- coding: utf-8 -*-
"""FoodVision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13y-BYv0NBTMAXsnplyrxrZZ9ffI_q3VQ
"""

from google.colab import files
files.upload()  # upload the kaggle.json file

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d evilspirit05/pizza-steak-sushi

!unzip pizza-steak-sushi.zip -d pizza_steak_sushi_data

!ls pizza_steak_sushi_data

!rm pizza-steak-sushi.zip

train_dir="pizza_steak_sushi_data/train"
test_dir="pizza_steak_sushi_data/test"

import torch
import torchvision
from torchvision import datasets
from torch.utils.data import DataLoader
from torch import nn
from torchvision import transforms

device="cuda" if torch.cuda.is_available() else "cpu"
device

# Manual creation for transforms
manual_transforms=transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406],
                         std=[0.229,0.224,0.225])
])

def create_dataloader(train_dir,test_dir,transform,batch_size,num_workers):

  #load datasets
   train_data=datasets.ImageFolder(train_dir,transform=transform)
   test_data=datasets.ImageFolder(test_dir,transform=transform)
  #get classes
   class_names=train_data.classes

   #turn images into dataloaders
   train_dataloader=DataLoader(train_data,batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True)
   test_dataloader=DataLoader(test_data,batch_size=batch_size,shuffle=False,num_workers=num_workers,pin_memory=True)

   return train_dataloader,test_dataloader,class_names

train,test,classes=create_dataloader(train_dir=train_dir,test_dir=test_dir,transform=manual_transforms,batch_size=32,num_workers=2)
train,test,classes

# Auto creation for transforms
weights=torchvision.models.EfficientNet_B0_Weights.DEFAULT
auto_transforms=weights.transforms()
auto_transforms

# we track weights and architecture of model
weights=torchvision.models.EfficientNet_B0_Weights.DEFAULT
effnet=torchvision.models.efficientnet_b0(weights=weights).to(device)
effnet

# we look at last layer of model to specify in_features and out_features

# print summary using torchinfo
!pip install torchinfo

from torchinfo import summary
summary(effnet,input_size=(32,3,224,224),col_names=["input_size","output_size","num_params","trainable"],col_width=20,row_settings=["var_names"])

#Freeze all base layers in the features section of the model by setting requires_grad=False
for name,param in effnet.named_parameters():
  if param.requires_grad:
    print(name)

for param in effnet.features.parameters():
  param.requires_grad=False

for name,param in effnet.named_parameters():
  if param.requires_grad:
    print(name)

torch.manual_seed(42)
torch.cuda.manual_seed(42)
output_shape=len(classes)

effnet.classifier=torch.nn.Sequential(
    torch.nn.Dropout(p=0.2,inplace=True),
    torch.nn.Linear(in_features=1280,out_features=output_shape,bias=True)
).to(device
)

summary(effnet,input_size=(32,3,224,224),col_names=["input_size","output_size","num_params","trainable"],col_width=20,row_settings=["var_names"])

# Define loss and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(effnet.parameters(), lr=0.001)

from tqdm.auto import tqdm #progress Bar

epochs = 5

# Training loop
for epoch in tqdm(range(epochs)):
    print(f"Epoch: {epoch}\n-------")
    ### Training
    train_loss = 0
    effnet.train()
    # Add a loop to loop through training batches
    for batch, (X, y) in enumerate(train):
        X, y = X.to(device), y.to(device)

        # 1. Forward pass
        y_pred = effnet(X)

        # 2. Calculate loss (per batch)
        loss = loss_fn(y_pred, y)
        train_loss += loss.item()

        # 3. Optimizer zero grad
        optimizer.zero_grad()

        # 4. Loss backward
        loss.backward()

        # 5. Optimizer step
        optimizer.step()

        # Print out how many samples have been seen
        if batch % 100 == 0:
            print(f"Looked at {batch * len(X)}/{len(train.dataset)} samples")

    train_loss /= len(train)

    ### Testing
    test_loss, test_acc = 0, 0
    effnet.eval()
    with torch.no_grad():
        for X, y in test:
            X, y = X.to(device), y.to(device)

            # 1. Forward pass
            test_pred_logits = effnet(X)

            # 2. Calculate loss (accumulatively)
            loss = loss_fn(test_pred_logits, y)
            test_loss += loss.item()

            # 3. Calculate accuracy
            test_pred_labels = test_pred_logits.argmax(dim=1)
            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))

        test_loss /= len(test)
        test_acc /= len(test)

    # Print out what's happening
    print(f"Train loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")

# Evaluate the model on the test set
def evaluate_model(model, dataloader, loss_fn, device):
    loss, acc = 0, 0
    model.eval()
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            y_pred = model(X)

            loss += loss_fn(y_pred, y).item()

            y_pred_label = y_pred.argmax(dim=1)
            acc += ((y_pred_label == y).sum().item()/len(y_pred_label))

        loss /= len(dataloader)
        acc /= len(dataloader)
    return loss, acc

test_loss, test_accuracy = evaluate_model(effnet, test, loss_fn, device)

print(f"Test loss: {test_loss:.5f}, Test accuracy: {test_accuracy:.2f}%")

from torchvision import transforms
from PIL import Image
import torch

# Prediction function
def predict_image(model, image_path, class_names, device):
    # Define same transforms as training/validation
    transform = transforms.Compose([
        transforms.Resize((224, 224)),   # EfficientNet input size
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],  # ImageNet mean
                             [0.229, 0.224, 0.225]) # ImageNet std
    ])

    # Load and preprocess image
    img = Image.open(image_path).convert("RGB")
    img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension

    # Set model to evaluation mode
    model.eval()
    with torch.no_grad():
        outputs = model(img_tensor)
        pred_idx = outputs.argmax(dim=1).item()
        prob = torch.softmax(outputs, dim=1)[0][pred_idx].item()

    return class_names[pred_idx], prob

# Suppose your dataset classes are like this
class_names = ["pizza", "steak", "sushi"]

# Example prediction
image_path = "/content/pizza_steak_sushi_data/test/sushi/1245193.jpg"   # replace with your image
label, probability = predict_image(effnet, image_path, class_names, device)

print(f"Predicted: {label} ({probability*100:.2f}%)")

# Save the entire model
torch.save(effnet.state_dict(), "effnet_pizza_steak_sushi.pth")

import torch
import torch.nn as nn
from torchvision import models

# Recreate the same model architecture
effnet = models.efficientnet_b0(pretrained=False)  # same architecture
effnet.classifier[1] = nn.Linear(in_features=1280, out_features=3)  # match your dataset classes

# Load weights
effnet.load_state_dict(torch.load("effnet_pizza_steak_sushi.pth", map_location="cpu"))
effnet.to(device)
effnet.eval()

from google.colab import files
uploaded = files.upload()  # choose an image from your computer

import matplotlib.pyplot as plt
from PIL import Image

def predict_and_show(model, image_path, class_names, device):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])

    # Load and preprocess
    img = Image.open(image_path).convert("RGB")
    img_tensor = transform(img).unsqueeze(0).to(device)

    # Prediction
    model.eval()
    with torch.no_grad():
        outputs = model(img_tensor)
        pred_idx = outputs.argmax(dim=1).item()
        prob = torch.softmax(outputs, dim=1)[0][pred_idx].item()

    # Show image with label
    plt.imshow(img)
    plt.axis("off")
    plt.title(f"Predicted: {class_names[pred_idx]} ({prob*100:.2f}%)")
    plt.show()

    return class_names[pred_idx], prob

class_names = ["pizza", "steak", "sushi"]

# Predict and show
label, prob = predict_and_show(effnet, "/content/download.jpg", class_names, device)
print(f"Predicted: {label} ({prob*100:.2f}%)")

#ViT model using Hugging face
from datasets import Dataset
import pandas as pd

labels_=['pizza','steak','sushi']
classNames=labels_
id2label={id:label for id,label in enumerate(classNames)}
label2id={label:id for id,label in id2label.items()}
id2label

o_train_path='/content/pizza_steak_sushi_data/train'
o_test_path='/content/pizza_steak_sushi_data/test'

train_path=[o_train_path+'/'+i for i in labels_]
test_path=[o_test_path+'/'+i for i in labels_]
train_path

import os
def create_Dataframe(paths):
  df_=pd.DataFrame(columns=['image_path','label'])
  for path in paths:
    list_img=[os.path.join(path,i) for i in os.listdir(path)]
    list_label=[label2id[path.split('/')[-1]] for i in range(len(list_img))]
    mydf=pd.DataFrame(list(zip(list_img,list_label)),columns=['image_path','label'])
    df_=pd.concat([df_,mydf],ignore_index=True)
    df_=df_.sample(frac=1).reset_index(drop=True)
  return df_

train_df=create_Dataframe(train_path)
test_df=create_Dataframe(test_path)
train_df

train_ds=Dataset.from_pandas(train_df)
test_ds=Dataset.from_pandas(test_df)
train_ds

import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2

from transformers import ViTFeatureExtractor

feature_extractor=ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')
feature_extractor

normalize=A.Normalize(mean=feature_extractor.image_mean,std=feature_extractor.image_std)

train_transforms = A.Compose([
    A.RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.9, 1.1)),
    A.HorizontalFlip(p=0.5),
    normalize,
    ToTensorV2()
])

val_transforms=A.Compose([
    A.Resize(224,224),
    A.CenterCrop(224,224),
    normalize,
    ToTensorV2()
])

def train_transforms_(examples):
  img_tensor_list=[]
  for ipath in examples['image_path']:
    im=cv2.imread(ipath)
    frame=cv2.cvtColor(im,cv2.COLOR_BGR2RGB)
    frame=train_transforms(image=frame)['image']
    img_tensor_list.append(frame)
    examples['pixel_values']=img_tensor_list
  return examples

def test_transforms_(examples):
  img_tensor_list=[]
  for ipath in examples['image_path']:
    im=cv2.imread(ipath)
    frame=cv2.cvtColor(im,cv2.COLOR_BGR2RGB)
    frame=val_transforms(image=frame)['image']
    img_tensor_list.append(frame)
    examples['pixel_values']=img_tensor_list
  return examples

print(train_ds.column_names)

train_ds.set_transform(train_transforms_)
test_ds.set_transform(test_transforms_)
train_ds[0]

from torch.utils.data import DataLoader
import torch

def collate_fn(examples):
  pixel_values=torch.stack([example["pixel_values"] for example in examples])
  labels=torch.tensor([example["label"] for example in examples])
  return {"pixel_values":pixel_values,"labels":labels}

train_dataloader=DataLoader(train_ds,collate_fn=collate_fn,batch_size=32,shuffle=True)
#test_dataloader=DataLoader(test_ds,collate_fn=collate_fn,batch_size=32,shuffle=False)

batch=next(iter(train_dataloader))
for k,v in batch.items():
  if isinstance(v,torch.Tensor):
    print(k,v.shape)

from transformers import ViTForImageClassification
model=ViTForImageClassification.from_pretrained('google/vit-base-patch16-224',num_labels=3,ignore_mismatched_sizes=True,id2label=id2label,label2id=label2id)

summary(model,input_size=(1,3,224,224),col_names=["input_size","output_size","num_params","trainable"],col_width=20,row_settings=["var_names"])

from transformers import TrainingArguments,Trainer
metric_name="accuracy"
args=TrainingArguments(
    f"foodVision_vit_trainer",
    save_strategy="epoch",
    eval_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
    remove_unused_columns=False,
    logging_dir="./logs",
    logging_steps=10,
    report_to="tensorboard"
)

pip install evaluate

import evaluate
import numpy as np
metric=evaluate.load('accuracy',trust_remote_code=True)
def compute_metric(eval_pred):
  predictions,labels=eval_pred
  predictions=np.argmax(predictions,axis=1)
  return metric.compute(predictions=predictions,references=labels)

trainer=Trainer(
    model,
    args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    data_collator=collate_fn,
    compute_metrics=compute_metric,
    tokenizer=feature_extractor
)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs/

train_output=trainer.train()

eval_results=trainer.evaluate()
eval_results

new_model_path='/content/foodVision_vit_trainer'
trainer.save_model(f"{new_model_path}/bestVIT")

model_out=ViTForImageClassification.from_pretrained('/content/foodVision_vit_trainer/bestVIT',num_labels=3,id2label=id2label,label2id=label2id)

os.makedirs("models", exist_ok=True)
torch.save(model_out,'models/pretrained_vit.pth')

from transformers import ViTFeatureExtractor, ViTForImageClassification
from PIL import Image
import torch

checkpoint_path = "/content/foodVision_vit_trainer/bestVIT"
processor = ViTFeatureExtractor.from_pretrained(checkpoint_path)
model = ViTForImageClassification.from_pretrained(checkpoint_path)
model.eval()

def predict_image(image_path, model, processor, id2label, device="cpu"):

    img = Image.open(image_path).convert("RGB")

    inputs = processor(images=img, return_tensors="pt").to(device)


    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        pred_id = logits.argmax(-1).item()
        prob = torch.softmax(logits, dim=-1)[0][pred_id].item()

    return id2label[pred_id], prob

image_path = "/content/download.jpg"
label, prob = predict_image(image_path, model, processor, id2label, device="cpu")

print(f"Predicted: {label} ({prob*100:.2f}%)")

model.to("cpu")
next(iter(model.parameters())).device

from timeit import default_timer as timer

from typing import Tuple,Dict
def predict(img)->Tuple[Dict,float]:
  start_time=timer()
  img=auto_transforms(img).unsqueeze(0)
  model.eval()
  with torch.inference_mode():
    outputs = model(img)
    pred_probs = torch.softmax(outputs.logits, dim=1)

  pred_labels_and_probs={class_names[i]:float(pred_probs[0][i])for i in range(len(class_names))}
  pred_time=round(timer()-start_time,5)
  return pred_labels_and_probs,pred_time

import random
from PIL import Image
from pathlib import Path
test_data_paths=list(Path(test_dir).glob("*/*.jpg"))
random_image_path=random.sample(test_data_paths,k=1)[0]
image=Image.open(random_image_path)
print(f"[INFO] predicting on images at path: {random_image_path}\n")
pred_dict,pred_time=predict(img=image)
print(f"prediction label and probability dictionary :\n{pred_dict}")
print(f"prediction time : {pred_time} second")

example_list=[[str(filepath)] for filepath in random.sample(test_data_paths,k=3)]
example_list

pip install gradio

import gradio as gr
title="Food Vision üçï üç£ ü•©"
description="This is app for detect 3 types of food"
demo=gr.Interface(
    fn=predict,
    inputs=gr.Image(type="pil"),
    outputs=[
             gr.Label(num_top_classes=3,label="Predictions"),
             gr.Number(label="Prediction time(s)")
    ],
    examples=example_list,
    title=title,
    description=description
)
demo.launch(debug=False,share=True)

import shutil
from pathlib import Path
foodvision_demo_path=Path("demos/foodvision/")
if foodvision_demo_path.exists():
  shutil.rmtree(foodvision_demo_path)
  foodvision_demo_path.mkdir(parents=True,exist_ok=True)
else:
  foodvision_demo_path.mkdir(parents=True,exist_ok=True)

!ls demos/foodvision/

foodvision_examples_path=foodvision_demo_path/"examples"
foodvision_examples_path.mkdir(parents=True,exist_ok=True)
foodvision_examples=[Path('/content/pizza_steak_sushi_data/test/pizza/1152100.jpg'),
                     Path('/content/pizza_steak_sushi_data/test/steak/1285886.jpg'),
                     Path('/content/pizza_steak_sushi_data/test/sushi/175783.jpg')]
for example in foodvision_examples:
  destination=foodvision_examples_path/example.name
  print(f"[INFO] Copying {example} to {destination}")
  shutil.copy2(src=example,dst=destination)

example_list=[["example/"+example]for example in os.listdir(foodvision_examples_path)]
example_list



viT_path="models/pretrained_vit.pth"
viT_dest=foodvision_demo_path/viT_path.split("/")[1]
try:
  print(f"[INFO] Attempting to move {viT_path} to {viT_dest}")
  shutil.copy2(viT_path,viT_dest)
except:
  print(f"[INFO] Unable to move {viT_path} to {viT_dest}")